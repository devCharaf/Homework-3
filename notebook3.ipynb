{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeed700d-688d-402d-b604-a40dece827fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94005e-0caf-4761-97d2-4dfc2ddee0d6",
   "metadata": {},
   "source": [
    "# Logistic regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f347ed9b-142a-4400-849a-26e1c2413c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self,\n",
    "                 data_path='Homework-3/data/',  # WRITE THE PATH TO YOUR DATA HERE\n",
    "                 optimizer='gd',\n",
    "                 gd_lr=0.001,\n",
    "                 eps=1e-8):\n",
    "        # Save the optimizer\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Save hyperparameter settings\n",
    "        self.gd_lr = gd_lr  # gd learning rate\n",
    "        self.eps = eps  # epsilon, for numerical stability\n",
    "\n",
    "        # Load the data\n",
    "        # self.df = pd.read_csv(os.path.join(data_path, 'bank/bank-full.csv'), sep=\";\")\n",
    "        self.df = pd.read_csv('data/bank/bank-full.csv', sep=\";\")\n",
    "\n",
    "        # Encoding data\n",
    "        cols = self.df.columns\n",
    "        num_cols = self.df._get_numeric_data().columns\n",
    "        cat_cols = list(set(cols) - set(num_cols))\n",
    "\n",
    "        ordinal_encoder = OrdinalEncoder()\n",
    "        self.df[cat_cols] = ordinal_encoder.fit_transform(self.df[cat_cols])\n",
    "\n",
    "        # split data\n",
    "        self.train_data, self.test_data = train_test_split(self.df, test_size=0.2, random_state=1)\n",
    "        self.train_data, self.val_data = train_test_split(self.train_data, test_size=0.25, random_state=1)\n",
    "\n",
    "        # labels\n",
    "        self.df_labels = self.df.y\n",
    "        self.train_labels = self.train_data.y\n",
    "        self.test_labels = self.test_data.y\n",
    "        self.val_labels = self.val_data.y\n",
    "\n",
    "        # drop the target value\n",
    "        self.train_data = self.train_data.drop(['y'], axis=1)\n",
    "        self.test_data = self.test_data.drop(['y'], axis=1)\n",
    "        self.val_data = self.val_data.drop(['y'], axis=1)\n",
    "\n",
    "        # transform back to numpy array\n",
    "        self.train_data = self.train_data.to_numpy()\n",
    "        self.test_data = self.test_data.to_numpy()\n",
    "        self.val_data = self.val_data.to_numpy()\n",
    "        self.train_labels = self.train_labels.to_numpy()\n",
    "        self.test_labels = self.test_labels.to_numpy()\n",
    "        self.val_labels = self.val_labels.to_numpy()\n",
    "\n",
    "        # Prepend a vector of all ones to each of the dataset\n",
    "        self.df[\"ones\"] = 1\n",
    "        self.train_data = np.hstack([np.ones_like(self.train_data[:, 0])[:, np.newaxis], self.train_data])\n",
    "        self.val_data = np.hstack([np.ones_like(self.val_data[:, 0])[:, np.newaxis], self.val_data])\n",
    "        self.test_data = np.hstack([np.ones_like(self.test_data[:, 0])[:, np.newaxis], self.test_data])\n",
    "\n",
    "        # Initialize the weight matrix\n",
    "        np.random.seed(seed=42)\n",
    "        self.start_w = np.random.rand(self.train_data.shape[1])\n",
    "\n",
    "        # Initialize the train logs\n",
    "        self.train_logs = {'train_accuracy': [], 'validation_accuracy': [], 'train_loss': [], 'validation_loss': []}\n",
    "\n",
    "    def sigmoid(self, a):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            v: float\n",
    "\n",
    "        returns:\n",
    "            the logistic sigmoid evaluated at a\n",
    "        \"\"\"\n",
    "        # WRITE CODE HERE\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "\n",
    "    def forward(self, w, X):\n",
    "        \"\"\"\n",
    "        inputs: w: an array of the current weights, of shape (d,)\n",
    "                X: an array of n datapoints, of shape (n, d)\n",
    "\n",
    "        outputs: an array of the output of the logistic regression (not 0s and 1s yet)\n",
    "        \"\"\"\n",
    "        # WRITE CODE HERE\n",
    "        return self.sigmoid(np.dot(X, w))\n",
    "\n",
    "    def loss(self, w, X, y):\n",
    "        \"\"\"\n",
    "        inputs: w: an array of the current weights, of shape (d,)\n",
    "                X: an array of n datapoints, of shape (n, d)\n",
    "\n",
    "        outputs: the loss. This is exactly the negative log likelihood\n",
    "        \"\"\"\n",
    "        # WRITE CODE HERE\n",
    "        # Note: add self.eps to a value before taking its log\n",
    "        E = 10 ** (-8)\n",
    "        y_pred = self.sigmoid(np.dot(X, w))\n",
    "        cost = -np.sum(y * np.log(y_pred + E) + (1 - y) * np.log(1 - y_pred + E))\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, w, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            w: an array of the current weights\n",
    "\n",
    "        returns:\n",
    "            an array representing the gradient of the loss\n",
    "        \"\"\"\n",
    "        # WRITE CODE HERE\n",
    "        grad = np.dot(X.T, (self.sigmoid(np.dot(X, w)) - y))\n",
    "        print(np.linalg.norm(grad))\n",
    "        return grad\n",
    "\n",
    "    def gd_step(self, w, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            w: an array of the current weights\n",
    "\n",
    "        returns:\n",
    "            a vector of weights updated according to a step of gradient descent\n",
    "            on the whole train dataset, using the learning rate self.gd_lr\n",
    "        \"\"\"\n",
    "        # WRITE CODE HERE\n",
    "        grad = self.gradient(w, X, y)\n",
    "        wnext = w - self.gd_lr * grad\n",
    "        return wnext\n",
    "\n",
    "    def compute_average_loss_and_accuracy(self, w, X, y):\n",
    "        outputs = self.forward(w, X)\n",
    "        predictions = np.array(np.round(outputs), dtype=int)\n",
    "        accuracy = np.mean(y == predictions)\n",
    "        loss = self.loss(w, X, y) / X.shape[0]\n",
    "        return loss, accuracy, predictions\n",
    "\n",
    "    def predict(self, X, w):\n",
    "        y_predict = self.forward(X, w)\n",
    "        return y_predict\n",
    "    \n",
    "    \n",
    "    def train_loop(self, n_epochs):\n",
    "        y_p = 1\n",
    "        N = 10\n",
    "        err = 0.1\n",
    "\n",
    "        # update constants\n",
    "        c = 1.1\n",
    "        c1 = 0.9\n",
    "        \n",
    "        y_sol = [y_p]\n",
    "        y_bar = [y_p]\n",
    "\n",
    "        # starting weight\n",
    "        w = np.array(np.copy(self.start_w), dtype=np.float128)\n",
    "\n",
    "        for k in range(n_epochs):\n",
    "\n",
    "            df = self.df.sample(n=N, random_state=1)\n",
    "            X = df.drop(['y'], axis=1).to_numpy()\n",
    "            y = df.y.to_numpy()\n",
    "\n",
    "\n",
    "            # solve\n",
    "            # if np.linalg.norm(self.gradient(w, X, y)) <= err:\n",
    "            if 1==1:\n",
    "#                 y = self.predict(w, X)\n",
    "#                 y_sol.append(y)\n",
    "\n",
    "#                 y_p = np.dot(y_sol, w) / np.sum(w)\n",
    "#                 y_bar.append(y_p)\n",
    "\n",
    "                df_loss, df_accuracy, _ = self.compute_average_loss_and_accuracy(w, X, y_p)\n",
    "                # print(\"accuracy:\", df_accuracy, '   loss: ', df_loss)\n",
    "\n",
    "                # updates\n",
    "                w = self.gd_step(w, X, y)\n",
    "            \n",
    "            # print(np.linalg.norm(self.gradient(w, X, y)))\n",
    "        \n",
    "            N *= c\n",
    "            N = int(N)\n",
    "            err *= c1\n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1327578-a38a-41c6-9a73-cc0a5ab44ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_loop(self, n_epochs):\n",
    "#         w = np.array(np.copy(self.start_w), dtype=np.float128)\n",
    "        \n",
    "#         c = 1.1\n",
    "#         N = 10\n",
    "#         subAlpha = 0.999\n",
    "#         ErrorThreshold = 0.1\n",
    "        \n",
    "#         # Choose an optimizer\n",
    "#         opt_step = self.gd_step\n",
    "\n",
    "#         for epoch in range(n_epochs):\n",
    "#             print(f\"epoch: {epoch},     sample size: {N}\")\n",
    "            \n",
    "#             df = self.df.sample(n=N, random_state=1)\n",
    "#             X = df.drop(['y'], axis=1).to_numpy()\n",
    "#             y = df.y.to_numpy()\n",
    "            \n",
    "#             ErrorPercent = 1.0\n",
    "            \n",
    "#             while(ErrorPercent >  ErrorThreshold):\n",
    "#                 for i in range(N):\n",
    "#                     batchX = X[i][:]\n",
    "#                     batchy = y[i]\n",
    "                    \n",
    "#                     cache = self.forward(w, batchX)\n",
    "#                     grads = self.gradient(w, batchX, batch)\n",
    "            \n",
    "#                 # GD\n",
    "#                 w = opt_step(w, X, y)\n",
    "\n",
    "#                 df_loss, df_accuracy, _ = self.compute_average_loss_and_accuracy(w, X, y)\n",
    "\n",
    "#                 # Changer l'erreur selon la sous-itération avec le sous échantions de cette itération.\n",
    "#                 ErrorPercent = 1.0 - df_accuracy \n",
    "#                 # Modifier le taux d'apprentissage.\n",
    "#                 if(ErrorPercent  >  ErrorThreshold):\n",
    "#                     subAlpha *= .98888888\n",
    "            \n",
    "#             if train_loss > previousLoss:\n",
    "#                 alpha  = alpha  *  .923\n",
    "#                 previousLoss = train_lossAll\n",
    "        \n",
    "#             # Augmenter le sampleSize et diminuer l'erreur voulu\n",
    "#             N += 100 \n",
    "        \n",
    "#             #  J'ai réaliser que ça prennait trop de temps a converger lorsque le seuil d'erreur devenait trop petit. \n",
    "#             if ErrorThreshold > 0.04:\n",
    "#                 ErrorThreshold  *= 95/100\n",
    "            \n",
    "            \n",
    "#             print(\"accuracy:\", df_accuracy, '   loss: ', df_loss)\n",
    "            \n",
    "#             # update sample\n",
    "#             # N = min(int(c * N), self.df.shape[0])\n",
    "        \n",
    "#         return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266e76e-a768-4a04-9bec-a35e8a49e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean squared error: sqrt(|y_predictinon^2 - y^2|)\n",
    "log likelihood: grad(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11616e4a-0c71-421e-8695-e5a32ac69611",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8f50a15-b1a9-4537-8544-7b791361bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11423.501520987336919\n",
      "450.01147080725475963\n",
      "450.00888880109932996\n",
      "450.0088888010991278\n",
      "450.0088888010991278\n",
      "450.0088888010991278\n",
      "702.85987223628011017\n",
      "702.85987223628011017\n",
      "702.85987223628011017\n",
      "1379.3367971601424655\n",
      "1379.3362117815227411\n",
      "1352.1338691120787098\n",
      "1352.1338691120787098\n",
      "2278.5124533344117757\n",
      "25356.465861787600467\n",
      "2727.7177273317706692\n",
      "2727.7177273317706692\n",
      "2727.7177260796980618\n",
      "5606.634819568686694\n",
      "5640.796131753034735\n",
      "5812.4277199806966716\n",
      "12823.917147268221432\n",
      "69704.12636279121485\n",
      "16208.098377045963624\n",
      "16179.4036045832047845\n",
      "17806.113276063364413\n",
      "17806.113276063364413\n",
      "102670.01410825634958\n",
      "25438.861609749757038\n",
      "26783.255067299045532\n",
      "26951.381652894903118\n",
      "31293.155881118797764\n",
      "149198.89081024426274\n",
      "47073.228378771728345\n",
      "50139.620730516101503\n",
      "49789.87294621266655\n",
      "185309.59712060247205\n",
      "76576.9981848335447\n",
      "76549.1102952869074\n",
      "76843.22709883031882\n",
      "328286.29833881279427\n",
      "82077.644428918645715\n",
      "89206.97450872325151\n",
      "97464.27099198967486\n",
      "319235.89294601149936\n",
      "110432.90575276917887\n",
      "122016.8352359624184\n",
      "124731.31480506408901\n",
      "719071.1324473434601\n",
      "167415.00972135085117\n",
      "191624.7014661731965\n",
      "208828.88650519592329\n",
      "237940.50242445063813\n",
      "1148236.8223276836952\n",
      "280264.3184995193069\n",
      "302388.16637395055312\n",
      "313932.17691724434\n",
      "342437.6926391135465\n",
      "1913100.3314993701605\n",
      "426783.50503153235152\n",
      "456545.04138255625128\n",
      "496544.59698903179185\n",
      "547381.56934902366146\n",
      "2979607.0388095043418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-54ea3bcd715b>:71: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-a))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675580.9442272628222\n",
      "720924.24711754563094\n",
      "788226.11943908582964\n",
      "885101.5916084916152\n",
      "4812624.3878519569535\n",
      "1052478.8159450051373\n",
      "1135269.6856769320078\n",
      "1226828.4821326899729\n",
      "1308533.852571569493\n",
      "7625887.9072688971696\n",
      "1609779.6842540285953\n",
      "1783178.5795516948167\n",
      "1936262.0892782051266\n",
      "2121522.098072277917\n",
      "11546238.0500584991205\n",
      "2528162.9160811610955\n",
      "2769716.1577912636353\n",
      "3037236.4196835253938\n",
      "3289914.764911243107\n",
      "19622109.36942735976\n",
      "4172191.6605259159642\n",
      "4568443.9948076412857\n",
      "5118207.658731423531\n",
      "5628188.234729919363\n",
      "3712727.7170039777586\n",
      "6373154.2566610609224\n",
      "38904009.266994256668\n",
      "8594568.746923838528\n",
      "9561285.87368524572\n",
      "10611243.398676942987\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4e03c5eeb250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mLR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Execution Time: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-54ea3bcd715b>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   5348\u001b[0m             )\n\u001b[1;32m   5349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5350\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # WRITE CODE HERE\n",
    "    # Instantiate, train, and evaluate your classifiers in the space below\n",
    "    LR = LogisticRegression()\n",
    "    start = time.time()\n",
    "    LR.train_loop(100)\n",
    "    end = time.time()\n",
    "    print('Execution Time: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d19b4f3-44e9-4e1d-9c30-e415168b99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    " def train_loop(self):\n",
    "        w = np.array(np.copy(self.start_w), dtype=np.float128)\n",
    "        \n",
    "        c = 1.1\n",
    "        N = 20\n",
    "        \n",
    "        # Choose an optimizer\n",
    "        opt_step = self.gd_step\n",
    "        \n",
    "        # Error\n",
    "        ErrorTreshold = 0.1\n",
    "        Error = 1\n",
    "        epoch = 0\n",
    "        \n",
    "        acc = []\n",
    "        err = []\n",
    "        ep = []\n",
    "        while Error > ErrorTreshold and N < self.df.shape[0]:\n",
    "        # for epoch in range(n_epochs):\n",
    "        \n",
    "            print(f\"epoch: {epoch},     sample size: {N}\")\n",
    "            epoch += 1\n",
    "            df = self.df.sample(n=N, random_state=1)\n",
    "            X = df.drop(['y'], axis=1).to_numpy()\n",
    "            y = df.y.to_numpy()\n",
    "            \n",
    "            # GD\n",
    "            w = opt_step(w, X, y)\n",
    "            \n",
    "            df_loss, df_accuracy, _ = self.compute_average_loss_and_accuracy(w, X, y)\n",
    "            Error = 1 - df_accuracy\n",
    "            print(f\"Error: {Error}\")\n",
    "            \n",
    "            err.append(df_loss)\n",
    "            acc.append(df_accuracy)\n",
    "            ep.append(epoch)\n",
    "            \n",
    "            print(\"accuracy:\", df_accuracy, '   loss: ', df_loss)\n",
    "            \n",
    "            # update sample\n",
    "            N = min(int(c * N), self.df.shape[0])\n",
    "            \n",
    "            if ErrorTreshold > 0.04:\n",
    "                ErrorTreshold  *= 95/100\n",
    "                \n",
    "        print(f\"itteration number: {epoch}\")\n",
    "        \n",
    "        \n",
    "        fig = px.scatter(x=ep, y=err)\n",
    "        fig.show()\n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05730ba-7d6f-4b37-be87-6022e0eaef16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
