{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea44bc33-12e8-48c4-8aac-aab29765c456",
   "metadata": {},
   "source": [
    "<center> <h1>Homework 3</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deabd511-4daa-4655-8e8c-e2667f57818d",
   "metadata": {},
   "source": [
    "This homework solution has been inspired from [IFT 6390](https://admission.umontreal.ca/cours-et-horaires/cours/ift-6390/) homeworks as well as the work of [Francois David](https://www.zspapapa.com/francoisdavid).\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "* To run this code, make sure you adjust the path to the .csv file in the 15th line of the second cell according to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70d05b-703d-49e6-a741-22e4e232a415",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4191a946-5f21-43d2-901f-0b1cbe399ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import timeit\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94005e-0caf-4761-97d2-4dfc2ddee0d6",
   "metadata": {},
   "source": [
    "# Logistic Regression and Retrospective Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f347ed9b-142a-4400-849a-26e1c2413c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self,\n",
    "                 optimizer='gd',\n",
    "                 gd_lr=0.001,\n",
    "                 eps=1e-8):\n",
    "        # Save the optimizer\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Save hyperparameter settings\n",
    "        self.gd_lr = gd_lr  # gd learning rate\n",
    "        self.eps = eps  # epsilon, for numerical stability\n",
    "\n",
    "        # Load the data\n",
    "        # self.df = pd.read_csv(os.path.join(data_path, 'bank/bank-full.csv'), sep=\";\")\n",
    "        self.df = pd.read_csv('data/bank/bank-full.csv', sep=\";\")\n",
    "\n",
    "        # Encoding data\n",
    "        cols = self.df.columns\n",
    "        num_cols = self.df._get_numeric_data().columns\n",
    "        cat_cols = list(set(cols) - set(num_cols))\n",
    "\n",
    "        ordinal_encoder = OrdinalEncoder()\n",
    "        self.df[cat_cols] = ordinal_encoder.fit_transform(self.df[cat_cols])\n",
    "\n",
    "        # split data into train, test and valid\n",
    "        self.train_data, self.test_data = train_test_split(self.df, test_size=0.2, random_state=1)\n",
    "        self.train_data, self.val_data = train_test_split(self.train_data, test_size=0.25, random_state=1)\n",
    "\n",
    "        # labels\n",
    "        self.train_labels = self.train_data.y\n",
    "        self.test_labels = self.test_data.y\n",
    "        self.val_labels = self.val_data.y\n",
    "\n",
    "        # drop the target value\n",
    "        self.train_data = self.train_data.drop(['y'], axis=1)\n",
    "        self.test_data = self.test_data.drop(['y'], axis=1)\n",
    "        self.val_data = self.val_data.drop(['y'], axis=1)\n",
    "\n",
    "        # transform back to numpy array\n",
    "        self.train_data = self.train_data.to_numpy()\n",
    "        self.test_data = self.test_data.to_numpy()\n",
    "        self.val_data = self.val_data.to_numpy()\n",
    "        self.train_labels = self.train_labels.to_numpy()\n",
    "        self.test_labels = self.test_labels.to_numpy()\n",
    "        self.val_labels = self.val_labels.to_numpy()\n",
    "\n",
    "        # Prepend a vector of all ones to each of the dataset\n",
    "        self.train_data = np.hstack([np.ones_like(self.train_data[:, 0])[:, np.newaxis], self.train_data])\n",
    "        self.val_data = np.hstack([np.ones_like(self.val_data[:, 0])[:, np.newaxis], self.val_data])\n",
    "        self.test_data = np.hstack([np.ones_like(self.test_data[:, 0])[:, np.newaxis], self.test_data])\n",
    "\n",
    "        # Initialize the weight matrix\n",
    "        np.random.seed(seed=42)\n",
    "        self.start_w = np.random.rand(self.train_data.shape[1])\n",
    "\n",
    "        # Initialize the train logs\n",
    "        self.train_logs = {'train_accuracy': [], 'validation_accuracy': [], 'train_loss': [], 'validation_loss': []}\n",
    "\n",
    "    def sigmoid(self, a: np.float128):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            v: float\n",
    "\n",
    "        returns:\n",
    "            the logistic sigmoid evaluated at a\n",
    "        \"\"\"\n",
    "        # we use the scipy sigmoid instead of np.exp() to avoid RuntimeWarning: overflow\n",
    "        return scipy.special.expit(a)\n",
    "        #Â return 1 / (1 + np.exp(-a))\n",
    "\n",
    "    def forward(self, w, X):\n",
    "        \"\"\"\n",
    "        inputs: w: an array of the current weights, of shape (d,)\n",
    "                X: an array of n datapoints, of shape (n, d)\n",
    "\n",
    "        outputs: an array of the output of the logistic regression (not 0s and 1s yet)\n",
    "        \"\"\"\n",
    "        return self.sigmoid(np.dot(X, w))\n",
    "\n",
    "    def loss(self, w, X, y):\n",
    "        \"\"\"\n",
    "        inputs: w: an array of the current weights, of shape (d,)\n",
    "                X: an array of n datapoints, of shape (n, d)\n",
    "\n",
    "        outputs: the loss. This is exactly the negative log likelihood\n",
    "        \"\"\"\n",
    "        E = 10 ** (-8)\n",
    "        y_pred = self.sigmoid(np.dot(X, w))\n",
    "        cost = -np.sum(y * np.log(y_pred + E) + (1 - y) * np.log(1 - y_pred + E))\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, w, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            w: an array of the current weights\n",
    "\n",
    "        returns:\n",
    "            an array representing the gradient of the loss\n",
    "        \"\"\"\n",
    "        grad = np.dot(X.T, (self.sigmoid(np.dot(X, w)) - y))\n",
    "        return grad\n",
    "\n",
    "    def gd_step(self, grad, w, alpha):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            w: an array of the current weights\n",
    "\n",
    "        returns:\n",
    "            a vector of weights updated according to a step of gradient descent\n",
    "            on the whole train dataset, using the learning rate self.gd_lr\n",
    "        \"\"\"\n",
    "        wnext = w - alpha * grad\n",
    "        return wnext\n",
    "\n",
    "    def compute_average_loss_and_accuracy(self, w, X, y):\n",
    "        outputs = self.forward(w, X)\n",
    "        predictions = np.array(np.round(outputs), dtype=int)\n",
    "        accuracy = np.mean(y == predictions)\n",
    "        loss = self.loss(w, X, y) / X.shape[0]\n",
    "        return loss, accuracy, predictions\n",
    "\n",
    "    def predict(self, X, w):\n",
    "        y_predict = self.forward(X, w)\n",
    "        return y_predict\n",
    "\n",
    "    def train_loopRetroApprox(self, iterations):\n",
    "        X = self.train_data\n",
    "        y = self.train_labels\n",
    "        \n",
    "        # starting weights\n",
    "        w = np.copy(self.start_w)\n",
    "        batch_size = 1\n",
    "        alpha = 0.9999\n",
    "\n",
    "        # starting sample size \n",
    "        sampleSize = 100 \n",
    "        iteration =  0\n",
    "        ErrorThreshold = 0.8\n",
    "        ErrorPercent = 1.0\n",
    "        wX =  {} \n",
    "        previousLoss = 2147483647\n",
    "\n",
    "        cumulIdx = 0\n",
    "        gradsOld = {}\n",
    "        ep = np.arange(iterations)\n",
    "        for iteration in range(iterations):\n",
    "            start = timeit.default_timer()\n",
    "            # Error reset \n",
    "            ErrorPercent = 1.0\n",
    "\n",
    "            Xt = self.train_data\n",
    "            yt = self.train_labels\n",
    "            X_trainS = Xt[:sampleSize][:]\n",
    "            y_trainS = yt[:sampleSize]\n",
    "\n",
    "            Xv = self.val_data\n",
    "            yv = self.val_labels\n",
    "            X_validS = Xv[:sampleSize][ :]\n",
    "            y_validS = yv[:sampleSize]\n",
    "\n",
    "            # Adjust the learning rate\n",
    "            subAlpha = alpha\n",
    "            p = 0\n",
    "            while(ErrorPercent >  ErrorThreshold):\n",
    "                gradsS = 0\n",
    "                \n",
    "                for i in range(sampleSize):\n",
    "                    batchX = X_trainS[1 * (i + cumulIdx):1 * (i + cumulIdx + 1), :]\n",
    "                    batchY = y_trainS[1 * (i + cumulIdx):1 * (i + cumulIdx + 1)]\n",
    "                    cumulIdx += 1 \n",
    "                    \n",
    "                    # To avoid index error\n",
    "                    if cumulIdx == X_trainS.shape[0]-1:\n",
    "                        cumulIdx = 0 \n",
    "\n",
    "                    loss = self.loss(w, batchX, batchY)\n",
    "\n",
    "                    # computing the average of the loss\n",
    "                    if i != 0:\n",
    "                        lossS += loss\n",
    "                    else:\n",
    "                        lossS = loss\n",
    "            \n",
    "                # Divide by the sampleSize\n",
    "                if iteration == 0 : \n",
    "                    lossOld = lossS * 1 / sampleSize\n",
    "                else : \n",
    "                    lossOld = (lossS + lossOld) / sampleSize * 2\n",
    "\n",
    "                # Update the weights\n",
    "                w = self.gd_step(lossOld, w, subAlpha)\n",
    "                \n",
    "                # Calculate the loss according to the sample\n",
    "                train_loss, train_accuracy, _ = self.compute_average_loss_and_accuracy(w, X_trainS, y_trainS)\n",
    "                valid_loss, valid_accuracy, _ = self.compute_average_loss_and_accuracy(w, X_validS, y_validS)\n",
    "                \n",
    "                # Check if the train_accuracy updates\n",
    "                if p == train_accuracy:\n",
    "                    print(f\"###################### {ErrorThreshold}\")\n",
    "                    break\n",
    "                \n",
    "                p = train_accuracy\n",
    "                \n",
    "                # Update error\n",
    "                ErrorPercent = 1.0 - train_accuracy\n",
    "                \n",
    "                # Update learning rate\n",
    "                if(ErrorPercent  >  ErrorThreshold):\n",
    "                    subAlpha *= .988888\n",
    "\n",
    "            # Stop timer\n",
    "            stop = timeit.default_timer()\n",
    "\n",
    "            # Compute the loss over the complete dataset\n",
    "            X_trainAll = self.train_data\n",
    "            y_trainAll = self.train_labels\n",
    "            train_lossAll, train_accuracyAll, _ = self.compute_average_loss_and_accuracy(w, X_trainAll, y_trainAll)\n",
    "            X_validAll = self.val_data\n",
    "            y_validAll = self.val_labels\n",
    "            valid_lossAll, valid_accuracyAll, _ = self.compute_average_loss_and_accuracy(w, X_validAll, y_validAll)\n",
    "\n",
    "            if train_loss > previousLoss:\n",
    "                alpha = alpha * .923\n",
    "            previousLoss = train_lossAll\n",
    "\n",
    "            # update sampleSize\n",
    "            sampleSize += 270\n",
    "\n",
    "            # ifnot, the while loop goes on forever\n",
    "            if ErrorThreshold > 0.18:\n",
    "                ErrorThreshold  *= .96\n",
    "            \n",
    "            # print loss and acuracy for each iteration\n",
    "            print(f\"Iteration {iteration} : Train Accuracy : {train_accuracyAll}, \\tValid  Accuracy : {valid_accuracyAll},\\t Train Loss : {train_lossAll}, \\tValid Loss :  {valid_lossAll} , Time : {stop - start}\")\n",
    "\n",
    "            self.train_logs['train_accuracy'].append(train_accuracyAll)\n",
    "            self.train_logs['validation_accuracy'].append(valid_accuracyAll)\n",
    "            self.train_logs['train_loss'].append(train_lossAll)\n",
    "            self.train_logs['validation_loss'].append(valid_lossAll)\n",
    "        \n",
    "        # ploting \n",
    "        fig = make_subplots(rows=2, cols=2)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(name=\"train_loss\", x=ep, y=self.train_logs['train_loss']),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(name=\"train_accuracy\", x=ep, y=self.train_logs['train_accuracy']),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(name=\"validation_loss\", x=ep, y=self.train_logs['validation_loss']),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(name=\"validation_accuracy\", x=ep, y=self.train_logs['validation_accuracy']),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=600, width=800, title_text=\"Convergence of the loss and accuracy\")\n",
    "        fig.show()\n",
    "\n",
    "        return self.train_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11616e4a-0c71-421e-8695-e5a32ac69611",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f50a15-b1a9-4537-8544-7b791361bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : Train Accuracy : 0.8495170684951707, \tValid  Accuracy : 0.847268303472683,\t Train Loss : 2.7700484388015902, \tValid Loss :  2.813418616466814 , Time : 0.0017883090000001545\n",
      "Iteration 1 : Train Accuracy : 0.8500700435007005, \tValid  Accuracy : 0.8477106834771069,\t Train Loss : 2.7605220951814444, \tValid Loss :  2.805179426044218 , Time : 0.007652535000000071\n",
      "Iteration 2 : Train Accuracy : 0.8502175035021751, \tValid  Accuracy : 0.8479318734793188,\t Train Loss : 2.7582965192491287, \tValid Loss :  2.8012003975375235 , Time : 0.01735769300000012\n",
      "Iteration 3 : Train Accuracy : 0.8504386935043869, \tValid  Accuracy : 0.8480424684804246,\t Train Loss : 2.75363283603414, \tValid Loss :  2.79760645613472 , Time : 0.020066099000000115\n",
      "Iteration 4 : Train Accuracy : 0.8505492885054929, \tValid  Accuracy : 0.8482636584826366,\t Train Loss : 2.752094543962966, \tValid Loss :  2.7950863685314657 , Time : 0.029183628000000184\n",
      "Iteration 5 : Train Accuracy : 0.8507336135073361, \tValid  Accuracy : 0.8482636584826366,\t Train Loss : 2.7492494830445366, \tValid Loss :  2.7941256328756414 , Time : 0.03041905999999983\n",
      "Iteration 6 : Train Accuracy : 0.8507704785077048, \tValid  Accuracy : 0.8483742534837425,\t Train Loss : 2.748909182744167, \tValid Loss :  2.7930501245762667 , Time : 0.033745383000000295\n",
      "Iteration 7 : Train Accuracy : 0.8508073435080734, \tValid  Accuracy : 0.8483742534837425,\t Train Loss : 2.747931141305231, \tValid Loss :  2.7919742334186797 , Time : 0.03518158100000068\n",
      "Iteration 8 : Train Accuracy : 0.8508442085084421, \tValid  Accuracy : 0.8484848484848485,\t Train Loss : 2.7467705444591646, \tValid Loss :  2.7903552186027976 , Time : 0.050214610999999465\n",
      "Iteration 9 : Train Accuracy : 0.8508810735088107, \tValid  Accuracy : 0.8484848484848485,\t Train Loss : 2.745976432459903, \tValid Loss :  2.787228050628177 , Time : 0.05586759500000049\n",
      "Iteration 10 : Train Accuracy : 0.8509179385091794, \tValid  Accuracy : 0.8488166334881664,\t Train Loss : 2.7452908665351887, \tValid Loss :  2.7835226664249655 , Time : 0.06380368200000053\n",
      "Iteration 11 : Train Accuracy : 0.8509916685099167, \tValid  Accuracy : 0.8489272284892723,\t Train Loss : 2.7448431213407534, \tValid Loss :  2.7828634448197342 , Time : 0.06309143200000022\n",
      "Iteration 12 : Train Accuracy : 0.8510285335102853, \tValid  Accuracy : 0.8489272284892723,\t Train Loss : 2.744157000178861, \tValid Loss :  2.782863278416429 , Time : 0.06435055499999986\n",
      "Iteration 13 : Train Accuracy : 0.8510285335102853, \tValid  Accuracy : 0.8489272284892723,\t Train Loss : 2.7440673479600965, \tValid Loss :  2.7828625839595444 , Time : 0.08058902800000034\n",
      "Iteration 14 : Train Accuracy : 0.8510285335102853, \tValid  Accuracy : 0.8489272284892723,\t Train Loss : 2.743643424725098, \tValid Loss :  2.7827792322683442 , Time : 0.08242316299999963\n",
      "Iteration 15 : Train Accuracy : 0.851065398510654, \tValid  Accuracy : 0.8489272284892723,\t Train Loss : 2.7419821556865664, \tValid Loss :  2.7823188144788973 , Time : 0.0852192219999992\n",
      "Iteration 16 : Train Accuracy : 0.85117599351176, \tValid  Accuracy : 0.8489272284892723,\t Train Loss : 2.7411028938091504, \tValid Loss :  2.7817409214835425 , Time : 0.08706134899999984\n",
      "Iteration 17 : Train Accuracy : 0.85117599351176, \tValid  Accuracy : 0.8489272284892723,\t Train Loss : 2.740117661070927, \tValid Loss :  2.781210497742182 , Time : 0.0970425240000008\n",
      "Iteration 18 : Train Accuracy : 0.8512497235124973, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.739218714582803, \tValid Loss :  2.7806025366289426 , Time : 0.11930436499999963\n",
      "Iteration 19 : Train Accuracy : 0.8512865885128659, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7388433127559586, \tValid Loss :  2.780056433372369 , Time : 0.10076104400000041\n",
      "Iteration 20 : Train Accuracy : 0.8512865885128659, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.73921360793952, \tValid Loss :  2.779526263529789 , Time : 0.11129360500000018\n",
      "Iteration 21 : Train Accuracy : 0.8512865885128659, \tValid  Accuracy : 0.8489272284892723,\t Train Loss : 2.7393532660023845, \tValid Loss :  2.7803251572433623 , Time : 0.11827759999999987\n",
      "Iteration 22 : Train Accuracy : 0.8512865885128659, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7391059193015703, \tValid Loss :  2.7808343734277843 , Time : 0.11123443500000008\n",
      "Iteration 23 : Train Accuracy : 0.8512865885128659, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7388571134323216, \tValid Loss :  2.7808261437431825 , Time : 0.11646062000000068\n",
      "Iteration 24 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7387244245145155, \tValid Loss :  2.7808260503022177 , Time : 0.12054360600000003\n",
      "Iteration 25 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738723189806176, \tValid Loss :  2.7808260494150607 , Time : 0.1291859449999997\n",
      "Iteration 26 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738723188434189, \tValid Loss :  2.7808260494056425 , Time : 0.1175875779999993\n",
      "Iteration 27 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738723188432866, \tValid Loss :  2.78082604940555 , Time : 0.1313539119999998\n",
      "Iteration 28 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7387231884263166, \tValid Loss :  2.7808260494055492 , Time : 0.12837911599999963\n",
      "Iteration 29 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738723187961373, \tValid Loss :  2.7808260494055492 , Time : 0.14481929699999974\n",
      "Iteration 30 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738723154302128, \tValid Loss :  2.7808260494055492 , Time : 0.13484373400000038\n",
      "Iteration 31 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7387206136618234, \tValid Loss :  2.7808260494055492 , Time : 0.149109041\n",
      "Iteration 32 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7386661329487443, \tValid Loss :  2.7808260494055492 , Time : 0.15677505200000041\n",
      "Iteration 33 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7385389851430357, \tValid Loss :  2.7808260494055492 , Time : 0.16151284700000001\n",
      "Iteration 34 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738406073939736, \tValid Loss :  2.7808260494055492 , Time : 0.17795268199999992\n",
      "Iteration 35 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738270568349827, \tValid Loss :  2.7808260494055492 , Time : 0.18420227100000108\n",
      "Iteration 36 : Train Accuracy : 0.8513234535132346, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738140079250881, \tValid Loss :  2.7808260494055492 , Time : 0.182433477\n",
      "Iteration 37 : Train Accuracy : 0.8513603185136032, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738054438619694, \tValid Loss :  2.7808260494055492 , Time : 0.182238761999999\n",
      "Iteration 38 : Train Accuracy : 0.8513603185136032, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7380444244901696, \tValid Loss :  2.7808260494055492 , Time : 0.196691006\n",
      "Iteration 39 : Train Accuracy : 0.8513603185136032, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738044118207223, \tValid Loss :  2.7808260494055492 , Time : 0.22337110099999968\n",
      "Iteration 40 : Train Accuracy : 0.8513603185136032, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738044109837437, \tValid Loss :  2.7808260494055492 , Time : 0.2087808710000001\n",
      "Iteration 41 : Train Accuracy : 0.8513603185136032, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.738044104012265, \tValid Loss :  2.7808260494055492 , Time : 0.21652059200000018\n",
      "Iteration 42 : Train Accuracy : 0.8513603185136032, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7380226737325914, \tValid Loss :  2.7808260494055492 , Time : 0.21441451899999997\n",
      "Iteration 43 : Train Accuracy : 0.8513603185136032, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.737715182202278, \tValid Loss :  2.7808260494055492 , Time : 0.21930440799999928\n",
      "Iteration 44 : Train Accuracy : 0.8513603185136032, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.737397741293405, \tValid Loss :  2.7808260494055492 , Time : 0.20320285799999915\n",
      "Iteration 45 : Train Accuracy : 0.8513971835139719, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.737361791036293, \tValid Loss :  2.7808260494055492 , Time : 0.21224904600000016\n",
      "Iteration 46 : Train Accuracy : 0.8513971835139719, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.737351052561301, \tValid Loss :  2.7808260494055492 , Time : 0.21524014399999913\n",
      "Iteration 47 : Train Accuracy : 0.8513971835139719, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7373202614626764, \tValid Loss :  2.7808260494055492 , Time : 0.22417810199999977\n",
      "Iteration 48 : Train Accuracy : 0.8513971835139719, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.7372636831790307, \tValid Loss :  2.7808260494055492 , Time : 0.22106509700000032\n",
      "Iteration 49 : Train Accuracy : 0.8513971835139719, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.737191032016418, \tValid Loss :  2.7808260494055492 , Time : 0.23805483699999996\n",
      "Iteration 50 : Train Accuracy : 0.8513971835139719, \tValid  Accuracy : 0.8490378234903783,\t Train Loss : 2.737112841718682, \tValid Loss :  2.7808260494055492 , Time : 0.2285348650000003\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    LR = LogisticRegression()\n",
    "    start = time.time()\n",
    "    LR.train_loopRetroApprox(100)\n",
    "    end = time.time()\n",
    "    print('Execution Time: ', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543020e2-f27d-42ba-9d67-12d839a68e2b",
   "metadata": {},
   "source": [
    "We can see that, at the start, this adapted \"Logistic Regression\" with the \"Retrospective Approximation\" quickly converges towards the optimum before moving rather constantly.\n",
    "\n",
    "On a few occasions, as the sample size increases, the algorithm takes a considerable amount of time to arrive at a solution (which satisfies the desired error threshold) depending on the new data in the sample with each iteration. Of course, the larger the sample, the more time it takes for an iteration to increase to achieve a desired level of performance. The iteration cost is therefore dependent on the new data in the sample to analyze. Because new samples less representative of the dataset have sometimes increased the optimal function that we wish to minimize, for which an \"Early-Stopping\" technique would be adequate if it is not possible to perform the function. 'algorithm for a long period of time in order to achieve the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86094a2-aa61-4b89-8b52-3454c825475a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
