{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeed700d-688d-402d-b604-a40dece827fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94005e-0caf-4761-97d2-4dfc2ddee0d6",
   "metadata": {},
   "source": [
    "# Logistic regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f347ed9b-142a-4400-849a-26e1c2413c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self,\n",
    "                 data_path='data/',  # WRITE THE PATH TO YOUR DATA HERE\n",
    "                 optimizer='gd',\n",
    "                 gd_lr=0.001,\n",
    "                 eps=1e-8):\n",
    "        # Save the optimizer\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Save hyperparameter settings\n",
    "        self.gd_lr = gd_lr  # gd learning rate\n",
    "        self.eps = eps  # epsilon, for numerical stability\n",
    "\n",
    "        # Load the data\n",
    "        self.df = pd.read_csv(os.path.join(data_path, 'bank/bank-full.csv'), sep=\";\")\n",
    "\n",
    "        # Encoding data\n",
    "        cols = self.df.columns\n",
    "        num_cols = self.df._get_numeric_data().columns\n",
    "        cat_cols = list(set(cols) - set(num_cols))\n",
    "\n",
    "        ordinal_encoder = OrdinalEncoder()\n",
    "        self.df[cat_cols] = ordinal_encoder.fit_transform(self.df[cat_cols])\n",
    "\n",
    "        # split data\n",
    "        self.train_data, self.test_data = train_test_split(self.df, test_size=0.2, random_state=1)\n",
    "        self.train_data, self.val_data = train_test_split(self.train_data, test_size=0.25, random_state=1)\n",
    "\n",
    "        # labels\n",
    "        self.train_labels = self.train_data.y\n",
    "        self.test_labels = self.test_data.y\n",
    "        self.val_labels = self.val_data.y\n",
    "\n",
    "        # drop the target value\n",
    "        self.train_data = self.train_data.drop(['y'], axis=1)\n",
    "        self.test_data = self.test_data.drop(['y'], axis=1)\n",
    "        self.val_data = self.val_data.drop(['y'], axis=1)\n",
    "\n",
    "        # transform back to numpy array\n",
    "        self.train_data = self.train_data.to_numpy()\n",
    "        self.test_data = self.test_data.to_numpy()\n",
    "        self.val_data = self.val_data.to_numpy()\n",
    "        self.train_labels = self.train_labels.to_numpy()\n",
    "        self.test_labels = self.test_labels.to_numpy()\n",
    "        self.val_labels = self.val_labels.to_numpy()\n",
    "\n",
    "        # Prepend a vector of all ones to each of the dataset\n",
    "        self.train_data = np.hstack([np.ones_like(self.train_data[:, 0])[:, np.newaxis], self.train_data])\n",
    "        self.val_data = np.hstack([np.ones_like(self.val_data[:, 0])[:, np.newaxis], self.val_data])\n",
    "        self.test_data = np.hstack([np.ones_like(self.test_data[:, 0])[:, np.newaxis], self.test_data])\n",
    "\n",
    "        # Initialize the weight matrix\n",
    "        np.random.seed(seed=42)\n",
    "        self.start_w = np.random.rand(self.train_data.shape[1])\n",
    "\n",
    "        # Initialize the train logs\n",
    "        self.train_logs = {'train_accuracy': [], 'validation_accuracy': [], 'train_loss': [], 'validation_loss': []}\n",
    "\n",
    "    def sigmoid(self, a):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            v: float\n",
    "\n",
    "        returns:\n",
    "            the logistic sigmoid evaluated at a\n",
    "        \"\"\"\n",
    "        # WRITE CODE HERE\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "\n",
    "    def forward(self, w, X):\n",
    "        \"\"\"\n",
    "        inputs: w: an array of the current weights, of shape (d,)\n",
    "                X: an array of n datapoints, of shape (n, d)\n",
    "\n",
    "        outputs: an array of the output of the logistic regression (not 0s and 1s yet)\n",
    "        \"\"\"\n",
    "        # WRITE CODE HERE\n",
    "        return self.sigmoid(np.dot(X, w))\n",
    "\n",
    "    def loss(self, w, X, y):\n",
    "        \"\"\"\n",
    "        inputs: w: an array of the current weights, of shape (d,)\n",
    "                X: an array of n datapoints, of shape (n, d)\n",
    "\n",
    "        outputs: the loss. This is exactly the negative log likelihood\n",
    "        \"\"\"\n",
    "        # WRITE CODE HERE\n",
    "        # Note: add self.eps to a value before taking its log\n",
    "        E = 10 ** (-8)\n",
    "        y_pred = self.sigmoid(np.dot(X, w))\n",
    "        cost = -np.sum(y * np.log(y_pred + E) + (1 - y) * np.log(1 - y_pred + E))\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, w, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            w: an array of the current weights\n",
    "\n",
    "        returns:\n",
    "            an array representing the gradient of the loss\n",
    "        \"\"\"\n",
    "        # WRITE CODE HERE\n",
    "        grad = np.dot(X.T, (self.sigmoid(np.dot(X, w)) - y))\n",
    "        return grad\n",
    "\n",
    "    def gd_step(self, w, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            w: an array of the current weights\n",
    "\n",
    "        returns:\n",
    "            a vector of weights updated according to a step of gradient descent\n",
    "            on the whole train dataset, using the learning rate self.gd_lr\n",
    "        \"\"\"\n",
    "        # WRITE CODE HERE\n",
    "        grad = self.gradient(w, X, y)\n",
    "        wnext = w - self.gd_lr * grad\n",
    "        return wnext\n",
    "\n",
    "    def compute_average_loss_and_accuracy(self, w, X, y):\n",
    "        outputs = self.forward(w, X)\n",
    "        predictions = np.array(np.round(outputs), dtype=int)\n",
    "        accuracy = np.mean(y == predictions)\n",
    "        loss = self.loss(w, X, y) / X.shape[0]\n",
    "        return loss, accuracy, predictions\n",
    "\n",
    "    def predict(self, X, w):\n",
    "        y_predict = self.forward(X, w)\n",
    "        return y_predict\n",
    "\n",
    "    def train_loop(self, n_epochs):\n",
    "        w = np.array(np.copy(self.start_w), dtype=np.float128)\n",
    "        X = self.train_data\n",
    "        y = self.train_labels\n",
    "\n",
    "        # Choose an optimizer\n",
    "        opt_step = self.gd_step\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # GD\n",
    "            w = opt_step(w, X, y)\n",
    "\n",
    "            train_loss, train_accuracy, _ = self.compute_average_loss_and_accuracy(w, self.train_data,\n",
    "                                                                                   self.train_labels)\n",
    "            valid_loss, valid_accuracy, _ = self.compute_average_loss_and_accuracy(w, self.val_data, self.val_labels)\n",
    "\n",
    "            self.train_logs['train_accuracy'].append(train_accuracy)\n",
    "            self.train_logs['validation_accuracy'].append(valid_accuracy)\n",
    "            self.train_logs['train_loss'].append(train_loss)\n",
    "            self.train_logs['validation_loss'].append(valid_loss)\n",
    "            \n",
    "            print(\"train_accuracy:\", train_accuracy, '   validation_accuracy: ', valid_accuracy, '   train_loss: ',train_loss, ' validation_loss: ' ,valid_loss )\n",
    "\n",
    "        # y_predict = self.predict(X, w)\n",
    "\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11616e4a-0c71-421e-8695-e5a32ac69611",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f50a15-b1a9-4537-8544-7b791361bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-6a71d502949a>:68: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-a))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy: 0.8175919781759198    validation_accuracy:  0.8167440831674408    train_loss:  3.360079926981357567  validation_loss:  3.3756987302454180405\n",
      "train_accuracy: 0.8174076531740765    validation_accuracy:  0.8166334881663349    train_loss:  3.3634753189952837568  validation_loss:  3.3777359654537737544\n",
      "train_accuracy: 0.817149598171496    validation_accuracy:  0.8156381331563813    train_loss:  3.368228867814780423  validation_loss:  3.3960710823289751797\n",
      "train_accuracy: 0.8163754331637544    validation_accuracy:  0.8146427781464278    train_loss:  3.38248951427327042  validation_loss:  3.414406199204176605\n",
      "train_accuracy: 0.8137580181375802    validation_accuracy:  0.8115461181154612    train_loss:  3.4307040808710223168  validation_loss:  3.4714487850381365952\n",
      "train_accuracy: 0.1759935117599351    validation_accuracy:  0.17827914178279142    train_loss:  15.178760449055639342  validation_loss:  15.1356593534978601575\n",
      "train_accuracy: 0.816522893165229    validation_accuracy:  0.8147533731475337    train_loss:  3.3797732006621294685  validation_loss:  3.4123689639958208913\n",
      "train_accuracy: 0.8161542431615424    validation_accuracy:  0.8140898031408981    train_loss:  3.3865639846899818482  validation_loss:  3.424592375245955175\n",
      "train_accuracy: 0.8150851581508516    validation_accuracy:  0.8128732581287326    train_loss:  3.4062572583707537493  validation_loss:  3.4470019625378680282\n",
      "train_accuracy: 0.8130575831305759    validation_accuracy:  0.8108825481088254    train_loss:  3.4436065705239418382  validation_loss:  3.4836721962882708792\n",
      "train_accuracy: 0.729189707291897    validation_accuracy:  0.7237336872373369    train_loss:  4.988509936860357839  validation_loss:  5.08901354047257346\n",
      "train_accuracy: 0.20043500700435007    validation_accuracy:  0.20194647201946472    train_loss:  14.728531898269519737  validation_loss:  14.700689253494831804\n",
      "train_accuracy: 0.8167809481678094    validation_accuracy:  0.8148639681486397    train_loss:  3.3750196518426328026  validation_loss:  3.4103317287874651774\n",
      "train_accuracy: 0.8165597581655976    validation_accuracy:  0.8146427781464278    train_loss:  3.3790941222593442303  validation_loss:  3.414406199204176605\n",
      "train_accuracy: 0.8156749981567499    validation_accuracy:  0.8132050431320504    train_loss:  3.3953920039261899417  validation_loss:  3.4408902569128008862\n",
      "train_accuracy: 0.8137211531372115    validation_accuracy:  0.8113249281132493    train_loss:  3.4313831592738075547  validation_loss:  3.4755232554548480233\n",
      "train_accuracy: 0.7378161173781612    validation_accuracy:  0.7331342623313426    train_loss:  4.8296055906086125522  validation_loss:  4.915848547762337775\n",
      "train_accuracy: 0.21905183219051833    validation_accuracy:  0.22107940721079408    train_loss:  14.3855968746024813825  validation_loss:  14.348247562449293294\n",
      "train_accuracy: 0.81703900317039    validation_accuracy:  0.8150851581508516    train_loss:  3.370266103023136137  validation_loss:  3.4062572583707537495\n",
      "train_accuracy: 0.8167440831674408    validation_accuracy:  0.8146427781464278    train_loss:  3.3756987302454180403  validation_loss:  3.414406199204176605\n",
      "train_accuracy: 0.8160436481604365    validation_accuracy:  0.8134262331342623    train_loss:  3.3886012198983375622  validation_loss:  3.4368157864960894585\n",
      "train_accuracy: 0.8141635331416354    validation_accuracy:  0.8114355231143552    train_loss:  3.423234218440384699  validation_loss:  3.4734860202464923093\n",
      "train_accuracy: 0.744046302440463    validation_accuracy:  0.7393275823932758    train_loss:  4.714841340537907334  validation_loss:  4.801763376094417795\n",
      "train_accuracy: 0.23221263732212638    validation_accuracy:  0.23733687237336873    train_loss:  14.143165884239912519  validation_loss:  14.048773986821004849\n",
      "train_accuracy: 0.8172970581729706    validation_accuracy:  0.8150851581508516    train_loss:  3.3655125542036394709  validation_loss:  3.4062572583707537495\n",
      "train_accuracy: 0.8170758681707587    validation_accuracy:  0.8149745631497456    train_loss:  3.3695870246203508988  validation_loss:  3.4082944935791094636\n",
      "train_accuracy: 0.816301703163017    validation_accuracy:  0.81431099314311    train_loss:  3.3838476710788408964  validation_loss:  3.4205179048292437471\n",
      "train_accuracy: 0.8152326181523262    validation_accuracy:  0.8122096881220969    train_loss:  3.4035409447596127975  validation_loss:  3.4592253737880023117\n",
      "train_accuracy: 0.7519354125193541    validation_accuracy:  0.7470692324706923    train_loss:  4.5695186033829136344  validation_loss:  4.6591569115095180344\n",
      "train_accuracy: 0.24452554744525548    validation_accuracy:  0.25193541251935414    train_loss:  13.916102707086969816  validation_loss:  13.779858939318049105\n",
      "train_accuracy: 0.8175182481751825    validation_accuracy:  0.8158593231585932    train_loss:  3.361438083786928043  validation_loss:  3.391996611912263752\n",
      "train_accuracy: 0.8174813831748138    validation_accuracy:  0.8156381331563813    train_loss:  3.362117162189713281  validation_loss:  3.3960710823289751797\n",
      "train_accuracy: 0.8169652731696527    validation_accuracy:  0.8150851581508516    train_loss:  3.3716242598287066124  validation_loss:  3.4062572583707537495\n",
      "train_accuracy: 0.8161542431615424    validation_accuracy:  0.8130944481309444    train_loss:  3.3865639846899818484  validation_loss:  3.4429274921211566003\n",
      "train_accuracy: 0.7584973825849738    validation_accuracy:  0.7518248175182481    train_loss:  4.44864261704680516  validation_loss:  4.5715557975502221204\n",
      "train_accuracy: 0.25654353756543535    validation_accuracy:  0.2685246626852466    train_loss:  13.6949741389698943575  validation_loss:  13.4742736580646920155\n",
      "train_accuracy: 0.8178500331785004    validation_accuracy:  0.816412298164123    train_loss:  3.3553263781618609012  validation_loss:  3.3818104358704851823\n",
      "train_accuracy: 0.8177025731770258    validation_accuracy:  0.8158593231585932    train_loss:  3.358042691773001853  validation_loss:  3.391996611912263752\n",
      "train_accuracy: 0.8174813831748138    validation_accuracy:  0.8155275381552753    train_loss:  3.3621176860408407481  validation_loss:  3.3981083175373308937\n",
      "train_accuracy: 0.8168915431689154    validation_accuracy:  0.814200398142004    train_loss:  3.3729824166342770883  validation_loss:  3.422555140037599461\n",
      "train_accuracy: 0.7638796726387967    validation_accuracy:  0.7600088476000885    train_loss:  4.3494971598394493013  validation_loss:  4.4208003921318992893\n",
      "train_accuracy: 0.269114502691145    validation_accuracy:  0.2798053527980535    train_loss:  13.463408403620128208  validation_loss:  13.266475666812409194\n",
      "train_accuracy: 0.8180343581803435    validation_accuracy:  0.8169652731696527    train_loss:  3.3519309861479347115  validation_loss:  3.3716242598287066126\n",
      "train_accuracy: 0.8181449531814495    validation_accuracy:  0.816301703163017    train_loss:  3.3498937509395789972  validation_loss:  3.3838476710788408964\n",
      "train_accuracy: 0.8181449531814495    validation_accuracy:  0.8160805131608051    train_loss:  3.3498937509395789972  validation_loss:  3.3879221414955523243\n",
      "train_accuracy: 0.8179974931799749    validation_accuracy:  0.8154169431541695    train_loss:  3.3526100645507199494  validation_loss:  3.4001455527456866076\n",
      "train_accuracy: 0.7681560126815601    validation_accuracy:  0.7653174076531741    train_loss:  4.2707240651196590494  validation_loss:  4.3230131021308250204\n",
      "train_accuracy: 0.28091130280911303    validation_accuracy:  0.29186020791860207    train_loss:  13.246089167697552952  validation_loss:  13.044417029101636803\n",
      "train_accuracy: 0.8183292781832928    validation_accuracy:  0.8172970581729706    train_loss:  3.3464983589256528077  validation_loss:  3.3655125542036394709\n",
      "train_accuracy: 0.8184398731843987    validation_accuracy:  0.8172970581729706    train_loss:  3.3444611237172970936  validation_loss:  3.3655125542036394709\n",
      "train_accuracy: 0.8184767381847674    validation_accuracy:  0.8169652731696527    train_loss:  3.3437820453145118559  validation_loss:  3.3716242598287066129\n",
      "train_accuracy: 0.8189928481899285    validation_accuracy:  0.8169652731696527    train_loss:  3.333747759848750154  validation_loss:  3.3716242598287066129\n",
      "train_accuracy: 0.7744599277445993    validation_accuracy:  0.7716213227162132    train_loss:  4.1546017125511911067  validation_loss:  4.2068906952545493273\n",
      "train_accuracy: 0.2911229079112291    validation_accuracy:  0.30103959301039596    train_loss:  13.057671166830412204  validation_loss:  12.875326506808112119\n",
      "train_accuracy: 0.818513603185136    validation_accuracy:  0.8178500331785004    train_loss:  3.3431029669117266175  validation_loss:  3.355326378161860901\n",
      "train_accuracy: 0.8188822531888226    validation_accuracy:  0.8178500331785004    train_loss:  3.336312182883874238  validation_loss:  3.355326378161860901\n",
      "train_accuracy: 0.8189191181891912    validation_accuracy:  0.8176288431762885    train_loss:  3.3356331044810889996  validation_loss:  3.359400848578572329\n",
      "train_accuracy: 0.8194720931947209    validation_accuracy:  0.8177394381773944    train_loss:  3.3254469284393104302  validation_loss:  3.357313530030848577\n",
      "train_accuracy: 0.7801371378013714    validation_accuracy:  0.7767086927670869    train_loss:  4.0500235844729170214  validation_loss:  4.1131778756701864856\n",
      "train_accuracy: 0.29974931799749316    validation_accuracy:  0.30933421809334216    train_loss:  12.899094250905595448  validation_loss:  12.7225338661814335725\n",
      "train_accuracy: 0.8187716581877166    validation_accuracy:  0.8179606281796062    train_loss:  3.3383494180922299518  validation_loss:  3.3532891429535051874\n",
      "train_accuracy: 0.8189928481899285    validation_accuracy:  0.8182924131829241    train_loss:  3.334274947675518524  validation_loss:  3.3471774373284380454\n",
      "train_accuracy: 0.8191771731917717    validation_accuracy:  0.8178500331785004    train_loss:  3.3308795556615923342  validation_loss:  3.3553263781618609012\n",
      "train_accuracy: 0.8199144731991447    validation_accuracy:  0.8180712231807122    train_loss:  3.3172979876058875744  validation_loss:  3.3512519077451494733\n",
      "train_accuracy: 0.7872889478728895    validation_accuracy:  0.7824596328245963    train_loss:  3.9182823742965811934  validation_loss:  4.0067589538714200553\n",
      "train_accuracy: 0.3037307380373074    validation_accuracy:  0.31320504313205044    train_loss:  12.825753783404789747  validation_loss:  12.651230633888983586\n",
      "train_accuracy: 0.8188085231880853    validation_accuracy:  0.8180712231807122    train_loss:  3.3376703396894447137  validation_loss:  3.3512519077451494735\n",
      "train_accuracy: 0.8192140381921403    validation_accuracy:  0.8181818181818182    train_loss:  3.3302004772588070958  validation_loss:  3.3492146725367937595\n",
      "train_accuracy: 0.8194720931947209    validation_accuracy:  0.8182924131829241    train_loss:  3.3254469284393158792  validation_loss:  3.3471774373284380456\n",
      "train_accuracy: 0.8202462582024626    validation_accuracy:  0.818624198186242    train_loss:  3.3111862819808204324  validation_loss:  3.3410657317033709036\n",
      "train_accuracy: 0.7922657229226572    validation_accuracy:  0.7880999778809997    train_loss:  3.8261125418325745064  validation_loss:  3.9033426492095479497\n",
      "train_accuracy: 0.3061638280616383    validation_accuracy:  0.31530634815306346    train_loss:  12.780935283610854774  validation_loss:  12.612523164930225021\n",
      "train_accuracy: 0.8189928481899285    validation_accuracy:  0.8182924131829241    train_loss:  3.334274947675518524  validation_loss:  3.3471774373284380454\n",
      "train_accuracy: 0.8192877681928776    validation_accuracy:  0.8182924131829241    train_loss:  3.32884232045323662  validation_loss:  3.3471774373284380454\n",
      "train_accuracy: 0.8195826881958269    validation_accuracy:  0.818513603185136    train_loss:  3.3234096932309547165  validation_loss:  3.3431029669117266175\n",
      "train_accuracy: 0.8203568532035685    validation_accuracy:  0.8190665781906658    train_loss:  3.308675925616352256  validation_loss:  3.3329167908699480478\n",
      "train_accuracy: 0.796394602963946    validation_accuracy:  0.789758902897589    train_loss:  3.7505500085879315837  validation_loss:  3.8727841210842122407\n",
      "train_accuracy: 0.30808080808080807    validation_accuracy:  0.31608051316080515    train_loss:  12.745622505156051561  validation_loss:  12.5982625184717350235\n",
      "train_accuracy: 0.819140308191403    validation_accuracy:  0.8184030081840301    train_loss:  3.3315586340643775721  validation_loss:  3.3451402021200823316\n",
      "train_accuracy: 0.819361498193615    validation_accuracy:  0.8182924131829241    train_loss:  3.3274841636476661442  validation_loss:  3.3471774373284380454\n",
      "train_accuracy: 0.8197670131976701    validation_accuracy:  0.8188453881884539    train_loss:  3.3200143012170285264  validation_loss:  3.3369912612866594757\n",
      "train_accuracy: 0.8209098282090983    validation_accuracy:  0.8189559831895599    train_loss:  3.2989628707306861489  validation_loss:  3.3349540260783037617\n",
      "train_accuracy: 0.8002654280026543    validation_accuracy:  0.7939615129396151    train_loss:  3.679246776290419417  validation_loss:  3.795369183166695111\n",
      "train_accuracy: 0.30999778809997786    validation_accuracy:  0.3175182481751825    train_loss:  12.71031045493129929  validation_loss:  12.571778460763110742\n",
      "train_accuracy: 0.8192877681928776    validation_accuracy:  0.8184030081840301    train_loss:  3.3288423204532366197  validation_loss:  3.3451402021200823316\n",
      "train_accuracy: 0.8194720931947209    validation_accuracy:  0.8181818181818182    train_loss:  3.3254469284393104304  validation_loss:  3.3492146725367937592\n",
      "train_accuracy: 0.8198776081987761    validation_accuracy:  0.8189559831895599    train_loss:  3.3179770660086728127  validation_loss:  3.3349540260783037617\n",
      "train_accuracy: 0.820836098208361    validation_accuracy:  0.818734793187348    train_loss:  3.300321027536256625  validation_loss:  3.3390284964950151898\n",
      "train_accuracy: 0.8033620880336209    validation_accuracy:  0.7966157929661579    train_loss:  3.6222041904564594268  validation_loss:  3.7464755381661579766\n",
      "train_accuracy: 0.3120622281206223    validation_accuracy:  0.3196195531961955    train_loss:  12.6722820643753259625  validation_loss:  12.533070991804352178\n",
      "train_accuracy: 0.819250903192509    validation_accuracy:  0.8184030081840301    train_loss:  3.3295213988560218579  validation_loss:  3.3451402021200823316\n",
      "train_accuracy: 0.8195089581950896    validation_accuracy:  0.8184030081840301    train_loss:  3.324767850036525192  validation_loss:  3.3451402021200823316\n",
      "train_accuracy: 0.8199513381995134    validation_accuracy:  0.8192877681928776    train_loss:  3.3166189092031023364  validation_loss:  3.3288423204532366201\n",
      "train_accuracy: 0.8206886382068864    validation_accuracy:  0.8189559831895599    train_loss:  3.3030373411473300293  validation_loss:  3.3349540260783037617\n",
      "train_accuracy: 0.8055371230553713    validation_accuracy:  0.7996018579960186    train_loss:  3.5821385647791076023  validation_loss:  3.6914701875405537003\n",
      "train_accuracy: 0.3144215881442159    validation_accuracy:  0.3224950232249502    train_loss:  12.628821046597070731  validation_loss:  12.480102876387103615\n",
      "train_accuracy: 0.8195089581950896    validation_accuracy:  0.818513603185136    train_loss:  3.324767850036525192  validation_loss:  3.3431029669117266177\n",
      "train_accuracy: 0.8196564181965642    validation_accuracy:  0.818624198186242    train_loss:  3.3220515364253842402  validation_loss:  3.3410657317033709034\n",
      "train_accuracy: 0.8202831232028313    validation_accuracy:  0.8192877681928776    train_loss:  3.3105072035780351947  validation_loss:  3.3288423204532366201\n",
      "train_accuracy: 0.8209835582098356    validation_accuracy:  0.8195089581950896    train_loss:  3.297604713925115673  validation_loss:  3.3247678500365251922\n",
      "Execution Time:  3.828184127807617\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # WRITE CODE HERE\n",
    "    # Instantiate, train, and evaluate your classifiers in the space below\n",
    "    LR = LogisticRegression()\n",
    "    start = time.time()\n",
    "    LR.train_loop(100)\n",
    "    end = time.time()\n",
    "    print('Execution Time: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05730ba-7d6f-4b37-be87-6022e0eaef16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
